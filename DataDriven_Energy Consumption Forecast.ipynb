{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schneider Electrics - Energy Consumption Forecast\n",
    "\n",
    "**End of competition: 31st March 23:59**\n",
    "\n",
    "*In the context of this challenge, we do not want to look at all of the details of the building--the objective of the challenge is to provide an algorithm that can*\n",
    "- *(i) either make a good forecast for all or some of the buildings*\n",
    "\n",
    "*or* \n",
    "- *(ii) bring the conclusion that other data would be necessary to make relevant forecasts.*\n",
    "\n",
    "https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/page/101/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the datasets & EDA\n",
    "https://www.drivendata.org/competitions/51/electricity-prediction-machine-learning/data/\n",
    "\n",
    "\n",
    "### Metadata of the buidlings (metadata.csv)\n",
    "- *SiteId*: ID of the building (range from 1 to 305 with missing values)\n",
    "- *Binary columns*: xx_IsDayOff from monday to sunday (regular days-off of the building)\n",
    "- *Numeric columns*: Surface, Sampling (number of minutes between each observation for this site), BaseTemperature (of the building in °C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the file\n",
    "metadata = pd.read_csv('metadata.csv')\n",
    "metadata['Sampling'] = metadata['Sampling'].astype('category')\n",
    "#Turning xxIsDayOff in dummy variables\n",
    "for lab in metadata.iloc[:, 4:].columns:\n",
    "    metadata[lab] = metadata[lab].map({False: 0, True:1})\n",
    "#Total days-off per week as categorical type\n",
    "metadata['Total_DaysOff'] = metadata.iloc[:, 4:].sum(axis=1).astype('category')\n",
    "\n",
    "print(metadata.head())\n",
    "print(metadata.tail())\n",
    "print(metadata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic visualizations of numerical values\n",
    "_ = metadata.plot(kind='scatter', x='SiteId', y='SiteId', figsize=(10,5))\n",
    "plt.show()\n",
    "\n",
    "_ = metadata.plot(kind='scatter', x='SiteId', y='Surface', figsize=(10,5))\n",
    "plt.show()\n",
    "\n",
    "_ = metadata.plot(kind='box', y='Surface', figsize=(5,10))\n",
    "plt.show()\n",
    "\n",
    "_ = metadata.plot(kind='scatter', x='SiteId', y='BaseTemperature', figsize=(10,5))\n",
    "plt.show()\n",
    "\n",
    "_ = metadata.plot(kind='bar', x='SiteId', y='Total_DaysOff', figsize=(10,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Holidays (holidays.csv)\n",
    "Public holidays at the sites included in the dataset, which may be helpful for identifying days where consumption may be lower than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the file\n",
    "holidays = pd.read_csv('holidays.csv', parse_dates=[1])\n",
    "holidays.index = holidays['Date']\n",
    "holidays = holidays.drop(['Unnamed: 0'], axis='columns')\n",
    "holidays['SiteId'] = holidays['SiteId'].astype('category')\n",
    "print(holidays.head())\n",
    "print(holidays.tail())\n",
    "print(holidays.info())\n",
    "\n",
    "#Days off count per building\n",
    "holidays_count = holidays.groupby('SiteId').count()\n",
    "\n",
    "_ = holidays_count['Holiday'].plot(kind='bar', figsize=(20, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Consumption - Training set (train.csv)\n",
    "\n",
    "\n",
    "- *obs_id*: Arbitrary ID for the observation\n",
    "- *SiteId*: ID number for the building that matches across datasets\n",
    "- *ForecastId*: ID for a timeseries that is part of a forecast (can be matched with the submission file)\n",
    "- *Timestamp*: The time of the measurement\n",
    "- *Value*: A measure of consumption for that building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the file\n",
    "training = pd.read_csv('train.csv', parse_dates=[2])\n",
    "training.index = training['Timestamp']\n",
    "#training = training.drop(['Timestamp'], axis=1)\n",
    "training['SiteId'] = training['SiteId'].astype('category')\n",
    "\n",
    "print(training.head())\n",
    "print(training.tail())\n",
    "print(training.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN values = 1.3% of the \"Value\" column\n",
    "print(len(training[training['Value'].isnull()]) / len(training))\n",
    "\n",
    "#Visualization of means for each building\n",
    "site_means = training.groupby('SiteId')['Value'].mean()\n",
    "_ = site_means.plot(kind='bar', figsize=(30,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evolution of power consumption for 1 building\n",
    "one_building = training[training['SiteId'] == 1][['Value', 'Timestamp']]\n",
    "one_building['week_day'] = one_building['Timestamp'].dt.dayofweek.astype('category')\n",
    "one_building['month'] = one_building['Timestamp'].dt.month.astype('category')\n",
    "#one_building['day_year'] = one_building['Timestamp'].dt.dayofyear.astype('category')\n",
    "print(one_building.head(10))\n",
    "\n",
    "#Mean according to day in the week\n",
    "day_mean = one_building.groupby('week_day')['Value'].mean()\n",
    "print(day_mean)\n",
    "\n",
    "#Mean according to month of the year\n",
    "month_mean = one_building.groupby('month')['Value'].mean()\n",
    "print(month_mean)\n",
    "\n",
    "#Power evolution\n",
    "_ = one_building.plot(y='Value', figsize=(20,5))\n",
    "plt.show()\n",
    "\n",
    "#Days-off for the SiteID 1 are Saturday and sunday\n",
    "_ = one_building['2014-09-04' : '2014-09-12'].plot(y='Value')\n",
    "plt.show()\n",
    "\n",
    "#Means\n",
    "_ = day_mean.plot()\n",
    "plt.show()\n",
    "_ = month_mean.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *For a buidling, the consumption seems to be higher in May-June-july (air conditionning ?)*\n",
    "- *The power consumption substantially drops during days-off of the building*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data (weather.csv)\n",
    "temperature data from several stations near each site. For each site several temperature measurements were retrieved from stations in a radius of 30 km if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv', index_col=0, parse_dates=[1])\n",
    "weather.index = weather['Timestamp']\n",
    "weather = weather.drop(['Timestamp'], axis=1)\n",
    "weather['SiteId'] = weather['SiteId'].astype('category')\n",
    "\n",
    "print(weather.head(30))\n",
    "print(weather.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temperature means per site\n",
    "mean_t_sites = weather.drop(columns=['Distance'])\n",
    "mean_t_sites = mean_t_sites.groupby('SiteId').mean()\n",
    "print(mean_t_sites.head(10))\n",
    "\n",
    "#Visualization\n",
    "_ = mean_t_sites.plot(kind='bar', figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--> Highly depends on the number of temperature records !*\n",
    "\n",
    "*--> No assumption that all the buildings are in the same city*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission format  \n",
    "The value is the target variable, for the given timestamp & site\n",
    "\n",
    "Same structure as the training dataset, useful to preprocess the submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pd.read_csv('submission_format.csv', parse_dates=[2])\n",
    "print(forecast.head())\n",
    "print(forecast.tail())\n",
    "print(forecast.info())\n",
    "\n",
    "building1 = forecast[forecast['SiteId'] == 1]\n",
    "print(\"\\n Site 1 \\n\", building1.head())\n",
    "print(building1.tail())\n",
    "building5 = forecast[forecast['SiteId'] == 5]\n",
    "print(\"\\n Site 5 \\n\", building5.head())\n",
    "print(building5.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training set preparation\n",
    "\n",
    "- Missing values for meaurements: fill with value in-between ? Drop as it's target variable?\n",
    "\n",
    "\n",
    "**FEATURES**: \n",
    "- Size of the flat, \n",
    "- dya of week and month of year (extracted from timestamp : https://pandas.pydata.org/pandas-docs/stable/api.html#datetimelike-properties ),\n",
    "- regular days-off, \n",
    "- national holidays, \n",
    "\n",
    "\n",
    "### Building of the full training set\n",
    "#### Preprocessing of the training dataset\n",
    "Saved under *'training_features_DB.csv'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All functions combined to get the full training dataset \n",
    "#besides temperatures at this point\n",
    "\n",
    "def add_features(file):\n",
    "    \"\"\"Adds to the train.csv or the submission_format.csv the corresponding features\n",
    "    except the temperature records\"\"\"\n",
    "    \n",
    "    \n",
    "    #Loading the original training dataset\n",
    "    training_preprocess = pd.read_csv(file, parse_dates=[2])\n",
    "    training_preprocess.index = training_preprocess['Timestamp']\n",
    "    training_preprocess['SiteId'] = training_preprocess['SiteId'].astype('category')\n",
    "    #print(training_preprocess.head())\n",
    "    #print(training_preprocess.info())\n",
    "\n",
    "\n",
    "    #Drop NaN values for the target variable\n",
    "    training_preprocess = training_preprocess.loc[training_preprocess['Value'].notnull()]\n",
    "\n",
    "\n",
    "    #Add 2 columns containing respectively the day of the week [0;6] and the month number [1;12] as features\n",
    "    training_preprocess['Day_Week'] = training_preprocess['Timestamp'].dt.dayofweek.astype('category')\n",
    "    training_preprocess['Month'] = training_preprocess['Timestamp'].dt.month.astype('category')\n",
    "\n",
    "\n",
    "    #Add national holidays as a feature according to the timestamps and the building Id    \n",
    "    #Loading holidays file\n",
    "    holidays = pd.read_csv('holidays.csv', parse_dates=[1])\n",
    "    holidays.index = holidays['Date']\n",
    "    holidays = holidays.drop(['Unnamed: 0'], axis='columns')\n",
    "    holidays.columns.values[0] = 'Timestamp'\n",
    "    holidays.columns.values[1] = 'National_Holiday'\n",
    "\n",
    "    #Merging the dataframes to add the corresponding National holidays of the building\n",
    "    training_preprocess = pd.merge(training_preprocess, holidays,\n",
    "                                   how='left', on=['Timestamp','SiteId'])\n",
    "\n",
    "\n",
    "    #Replace NaN by 0 and values by 1 to have a dummy variable\n",
    "    training_preprocess.loc[training_preprocess['National_Holiday'].notnull(), 'National_Holiday'] = 1\n",
    "    training_preprocess['National_Holiday'] = training_preprocess['National_Holiday'].fillna(0)\n",
    "\n",
    "\n",
    "    #Get dummy features\n",
    "    training_preprocess = pd.get_dummies(training_preprocess)\n",
    "\n",
    "\n",
    "    #Add surface, sampling frequency(min) & base temperature of the building according to SiteId from metadata\n",
    "    #Loading metadata\n",
    "    metadata = pd.read_csv('metadata.csv')\n",
    "\n",
    "    #Merging dataframes\n",
    "    training_preprocess = pd.merge(training_preprocess, \n",
    "                                   metadata[['SiteId', 'Surface', 'Sampling', 'BaseTemperature']],\n",
    "                                   how='left', on=['SiteId'])\n",
    "\n",
    "    \n",
    "    print(training_preprocess.head(20))\n",
    "    print(training_preprocess.info())\n",
    "    \n",
    "    training_preprocess.to_csv('{}_features_DB.csv'.format(file[:-4]))\n",
    "    \n",
    "    return training_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features to the training set\n",
    "add_features('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Sampling column is not a feature but an help to get the correct temperature up/down sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features to the submission csv\n",
    "add_features('submission_format.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Site with most records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of records per site to pick up the biggest one for machine learning\n",
    "records_count = training_preprocess.groupby('SiteId').count()['Value'].sort_values(ascending=False)\n",
    "\n",
    "_ = records_count.plot(kind='bar', figsize=(20,5))\n",
    "plt.show()\n",
    "\n",
    "print(records_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SiteId = 162 has 53 687 records, enough to train the ML model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the features vs consumption for 1 building\n",
    "\n",
    "- Difference between regular day-off and national holiday, especially if the heating/air conditionning is scheduled and do not take into account national holidays\n",
    "- The feature is not temperature but the difference betwen base temp and outdoor temp (absolute value?), containing 2 information instead of one, and probably trigerring the heating/air conditionning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_preprocess = pd.read_csv('train_features_DB.csv', index_col=0, parse_dates=[3])\n",
    "training_preprocess['SiteId'] = training_preprocess['SiteId'].astype('int')\n",
    "print(training_preprocess.info())\n",
    "\n",
    "\n",
    "def building_features(SiteId, viz=False):\n",
    "    \"\"\"For he specified building, creates a training set with the features from\n",
    "    the weather and metadata csv files\n",
    "    If vis=True, enables plotting of consumption means and temperature evolution\"\"\"\n",
    "\n",
    "    dataset = training_preprocess.loc[training_preprocess['SiteId'] == SiteId]\n",
    "    dataset.index = dataset['Timestamp']\n",
    "    #print(dataset.head())\n",
    "    #print(dataset.info())\n",
    "\n",
    "    if viz == True:\n",
    "        #Evolution consumption\n",
    "        _ = dataset.plot(y='Value', figsize=(20,10), \n",
    "                         title=\"Consumption records of the building {}\".format(SiteId))\n",
    "        plt.show()\n",
    "\n",
    "        #Subplots of consumption on all days of the week\n",
    "        means = dict()\n",
    "        for i in range(7):\n",
    "            day = 'Day_Week_' + str(i)\n",
    "            means[day] = dataset[dataset[day] == 1]['Value'].mean()\n",
    "        means['National_holiday'] = dataset[dataset['National_Holiday'] == 1]['Value'].mean()\n",
    "        _ = pd.Series(means).plot(kind='bar', figsize=(20,10),\n",
    "                                 title=\"Consumption mean for days of week for building {}\".format(SiteId))\n",
    "        plt.show()\n",
    "\n",
    "        #Subplots of consumption on months\n",
    "        means = dict()\n",
    "        for i in range(1,13):\n",
    "            month = 'Month_' + str(i)\n",
    "            means[month] = dataset[dataset[month] == 1]['Value'].mean()\n",
    "        _ = pd.Series(means).plot(kind='bar', figsize=(20,10),\n",
    "                                 title=\"Consumption mean for months for building {}\".format(SiteId))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    #Diff Temp vs consumption\n",
    "    #Get temperatue with the righ sampling\n",
    "    weather = pd.read_csv('weather.csv', index_col=0, parse_dates=[1])\n",
    "    weather = weather.loc[weather['SiteId'] == SiteId]\n",
    "    weather.index = weather['Timestamp']\n",
    "    weather = weather.drop(['Distance'], axis=1)\n",
    "    weather = weather.drop(['Timestamp', 'SiteId'], axis=1)\n",
    "\n",
    "    if viz == True:\n",
    "        #Plot temperatures\n",
    "        _ = weather.plot(figsize=(20,10), \n",
    "                         title=\"Temperature records for building {}\".format(SiteId))\n",
    "        plt.show()\n",
    "        \n",
    "    #Resample the temperatures to match the energy consumption in the training set\n",
    "    SAMP = str(int(dataset['Sampling'].values[0])) + 'min'\n",
    "    weather_resampled = weather['Temperature'].resample(SAMP).first().interpolate('linear')\n",
    "    #print(weather_resampled.head())\n",
    "    #print(dataset.head())\n",
    "\n",
    "    #Merge the resampled temperature to the training set\n",
    "    dataset = dataset.join(weather_resampled)\n",
    "\n",
    "    #Get the difference from the Base Temperature\n",
    "    BASET = dataset['BaseTemperature'].values[0]\n",
    "    dataset['Diff_Temp'] = dataset['Temperature'] - BASET\n",
    "\n",
    "    #Add the hour of day feature\n",
    "    dataset['Hour_Day'] = dataset['Timestamp'].dt.hour.astype('category')\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    #Drop non feature columns from the training set\n",
    "    dataset = dataset.drop(columns=['BaseTemperature', 'Temperature', 'Sampling','Surface', \n",
    "                                    'ForecastId','Timestamp', 'SiteId', 'obs_id'])\n",
    "\n",
    "    #Drop extra dummy features (1 in day, 1 in month & 1 in hours of day)\n",
    "    dataset = dataset.drop(columns=['Day_Week_0', 'Month_1', 'Hour_Day_0'])\n",
    "\n",
    "    dataset.to_csv('training_building{}_DB.csv'.format(SiteId))\n",
    "\n",
    "    #print(dataset.head(20))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_features(162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*--> The consumption is reduced on sundays and national holidays compared to weekdays and saturday (even if holiday too)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_features(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "building_features(302)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the csv training sets for all buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SITES = training_preprocess['SiteId'].unique().tolist()\n",
    "#print(SITES)\n",
    "\n",
    "for site in SITES:\n",
    "    print(\"Building {}\".format(site))\n",
    "    building_features(site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models\n",
    "Target: energy consumption forecast for each building\n",
    "\n",
    "- Preprocessing: \n",
    "    - getting data from metadata globally\n",
    "    - getting temperatures for one given building\n",
    "    - dummy variable for site ID, Imputing, Scaling\n",
    "\n",
    "- Hashing for computing efifciency\n",
    "\n",
    "- Regression Models: Ridge, Lasso, ElasticNet\n",
    "\n",
    "- Hyperparameter tuning with gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SImple Model for 1 building\n",
    "- Training set: see above for 1 building\n",
    "- Features: day of week, month, dayoff, national holiday,  /\\temperature\n",
    "- Preprocessing: scaling (temperature), imputing?, dummy variables (already done)\n",
    "- Model: Simple Ridge regression (hyperparameter = alpha)\n",
    "\n",
    "*Test with the building 162*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for preprocessing/addind features\n",
    "- Training & Submission sets: only Obs_Id, Timestamp, SiteId, ForecastId, Value\n",
    "- Features to add:\n",
    "    - Temperature: \n",
    "    - Days off (regular & bank-holidays)\n",
    "    - Day of month, hour of day\n",
    "    \n",
    "Creates function to go into the pieline using FunctionTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, ElasticNet, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SiteId = 162\n",
    "\n",
    "#Load training set\n",
    "training = pd.read_csv('training_building{}_DB.csv'.format(SiteId), index_col=0)\n",
    "print(training.head())\n",
    "print(training.info())\n",
    "#print(training.columns)\n",
    "\n",
    "#Create the features (X) and target (y) arrays\n",
    "y = training.loc[:,'Value'].values.reshape((-1,1))\n",
    "X = training.drop(columns=['Value']).values\n",
    "\n",
    "print('shape of X and Y arrays: {}'.format([X.shape, y.shape]))\n",
    "\n",
    "\n",
    "#Building the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "#Alpha Hyperparameter tunning\n",
    "param_grid={'reg__alpha': np.logspace(-4,0,50)}\n",
    "\n",
    "#Pipeline building\n",
    "pipeline = Pipeline([('reg', Ridge(normalize=True))\n",
    "                    ])\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=param_grid, cv=10)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "predictions = cv.predict(X_test)\n",
    "\n",
    "print(cv.best_params_)\n",
    "print(cv.best_score_)\n",
    "print('Score R² on test set: {}'.format(cv.score(X_test, y_test)))\n",
    "\n",
    "#Simple RMSE for 1 building\n",
    "mu = 1 / np.mean(y_test)\n",
    "simple_rmse = mu * np.sqrt(np.sum((predictions - y_test) ** 2))\n",
    "print('RMSE result: {}'.format(simple_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Best results with Ridge:**\n",
    "*cv=10  |  normalize=True  |  {'reg__alpha': 0.0001}  |  Best obtained R²: 0.7683974571210929  |  Score R² on test set: 0.77008  |  RMSE = 23.26860\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SiteId = 1\n",
    "\n",
    "#Load training set\n",
    "training = pd.read_csv('training_building{}_DB.csv'.format(SiteId), index_col=0)\n",
    "print(training.head())\n",
    "print(training.info())\n",
    "print(training.columns)\n",
    "\n",
    "#Create the features (X) and target (y) arrays\n",
    "y = training.loc[:,'Value'].values.reshape((-1,1))\n",
    "X = training.drop(columns=['Value']).values\n",
    "\n",
    "print('shape of X and Y arrays: {}'.format([X.shape, y.shape]))\n",
    "\n",
    "\n",
    "#Building the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "#Alpha Hyperparameter tunning\n",
    "param_grid={'reg__alpha': np.logspace(-4,0,50)}\n",
    "\n",
    "#Pipeline building\n",
    "pipeline = Pipeline([('reg', Ridge(normalize=True))\n",
    "                    ])\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=param_grid, cv=10)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "predictions = cv.predict(X_test)\n",
    "\n",
    "print(cv.best_params_)\n",
    "print(cv.best_score_)\n",
    "print('Score R² on test set: {}'.format(cv.score(X_test, y_test)))\n",
    "\n",
    "#Simple RMSE for 1 building\n",
    "mu = 1 / np.mean(y_test)\n",
    "simple_rmse = mu * np.sqrt(np.sum((predictions - y_test) ** 2))\n",
    "print('RMSE result: {}'.format(simple_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SiteId = 302\n",
    "\n",
    "#Load training set\n",
    "training = pd.read_csv('training_building{}_DB.csv'.format(SiteId), index_col=0)\n",
    "print(training.head())\n",
    "print(training.info())\n",
    "print(training.columns)\n",
    "\n",
    "#Create the features (X) and target (y) arrays\n",
    "y = training.loc[:,'Value'].values.reshape((-1,1))\n",
    "X = training.drop(columns=['Value']).values\n",
    "\n",
    "print('shape of X and Y arrays: {}'.format([X.shape, y.shape]))\n",
    "\n",
    "\n",
    "#Building the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "#Alpha Hyperparameter tunning\n",
    "param_grid={'reg__alpha': np.logspace(-4,0,50)}\n",
    "\n",
    "#Pipeline building\n",
    "pipeline = Pipeline([('reg', Ridge(normalize=True))\n",
    "                    ])\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=param_grid, cv=10)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "predictions = cv.predict(X_test)\n",
    "\n",
    "print(cv.best_params_)\n",
    "print(cv.best_score_)\n",
    "print('Score R² on test set: {}'.format(cv.score(X_test, y_test)))\n",
    "\n",
    "#Simple RMSE for 1 building\n",
    "mu = 1 / np.mean(y_test)\n",
    "simple_rmse = mu * np.sqrt(np.sum((predictions - y_test) ** 2))\n",
    "print('RMSE result: {}'.format(simple_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for the submission csv, predictions on buildings individually\n",
    "Pipeline that does predictions for each building individually (rather than using the entire dataset to make predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_features_sub(SiteId, dataset):\n",
    "    \"\"\"For the specified building in the submission format file (filtered submission), \n",
    "    creates a set for predictions with the features from\n",
    "    the weather (NOT RESAMPLED HERE) and metadata csv files\"\"\"\n",
    "    \n",
    "\n",
    "    #Diff Temp vs consumption\n",
    "    #Get temperatue with the righ sampling\n",
    "    weather = pd.read_csv('weather.csv', index_col=0, parse_dates=[1])\n",
    "    weather = weather.loc[weather['SiteId'] == SiteId]\n",
    "    weather.index = weather['Timestamp']\n",
    "    weather = weather.drop(['Distance'], axis=1)\n",
    "    weather = weather.drop(['Timestamp', 'SiteId'], axis=1)\n",
    "\n",
    "    \n",
    "    #Resample the temperatures to match the energy consumption in the training set\n",
    "    SAMP = str(int(dataset['Sampling'].values[0])) + 'min'\n",
    "    weather_resampled = weather['Temperature'].resample('D').mean()#.first().interpolate('linear')\n",
    "    time_index = dataset.index\n",
    "    weather_resampled = weather_resampled.reindex(dataset.index)\n",
    "        \n",
    "    #print(weather_resampled.head())\n",
    "    #print(dataset.head())\n",
    "\n",
    "    #Merge the resampled temperature to the training set\n",
    "    dataset['Temperature'] = weather_resampled\n",
    "\n",
    "    #Get the difference from the Base Temperature\n",
    "    BASET = dataset['BaseTemperature'].values[0]\n",
    "    dataset['Diff_Temp'] = dataset['Temperature'] - BASET\n",
    "\n",
    "    #Add the hour of day feature\n",
    "    dataset['Hour_Day'] = dataset['Timestamp'].dt.hour.astype('category')\n",
    "    dataset = pd.get_dummies(dataset)\n",
    "\n",
    "    #Drop non feature columns from the training set\n",
    "    dataset = dataset.drop(columns=['BaseTemperature', 'Temperature', 'Sampling','Surface', \n",
    "                                    'ForecastId','Timestamp', 'SiteId', 'obs_id'])\n",
    "\n",
    "    #Drop extra dummy features (1 in day, 1 in month & 1 in hours of day)\n",
    "    dataset = dataset.drop(columns=['Day_Week_0', 'Month_1', 'Hour_Day_0'])\n",
    "\n",
    "    #dataset.to_csv('training_building{}_DB.csv'.format(SiteId))\n",
    "\n",
    "    #print(dataset.head(20))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildings in the Pipeline: [1, 2, 3]\n",
      "\n",
      " Training of model for building 1\n",
      "\n",
      "Hyperparameter fine tunning: {'reg__alpha': 0.14873521072935117} 0.12619865672087166\n",
      "Score R² on test set: 0.21524455098354686\n",
      "RMSE result of training: 7.60757674997267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training of model for building 2\n",
      "\n",
      "Hyperparameter fine tunning: {'reg__alpha': 0.0003562247890262444} 0.4401874988322144\n",
      "Score R² on test set: 0.44706458816892003\n",
      "RMSE result of training: 49.85531717773496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training of model for building 3\n",
      "\n",
      "Hyperparameter fine tunning: {'reg__alpha': 0.01610262027560939} 0.7152283860790387\n",
      "Score R² on test set: 0.7909868619622098\n",
      "RMSE result of training: 2.0934737231634966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:466: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Damien\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             obs_id  SiteId  Timestamp  ForecastId         Value\n",
      "Timestamp                                                       \n",
      "2015-08-29  1677832       1 2015-08-29           1  3.270972e+06\n",
      "2015-08-30  5379616       1 2015-08-30           1  3.494990e+06\n",
      "2015-08-31   496261       1 2015-08-31           1  4.286233e+06\n",
      "2015-09-01  4567147       1 2015-09-01           1  4.637941e+06\n",
      "2015-09-02  3684873       1 2015-09-02           1  4.275754e+06\n",
      "   Building        R2       RMSE\n",
      "0         1  0.215245   7.607577\n",
      "1         2  0.447065  49.855317\n",
      "2         3  0.790987   2.093474\n",
      "Best performances of the model: R²=0.7909868619622098 and RMSE=2.0934737231634966\n",
      "Wall time: 6min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "submission = pd.read_csv('submission_format_features_DB.csv', parse_dates=[3], index_col=0)\n",
    "submission.index = submission['Timestamp']\n",
    "#print(submission.head())\n",
    "#print(submission.info())\n",
    "\n",
    "SUB = ['obs_id', 'SiteId', 'Timestamp', 'ForecastId']\n",
    "\n",
    "SITES = submission['SiteId'].unique().tolist()[:3]\n",
    "print(\"Buildings in the Pipeline: {}\".format(SITES))\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "predictions_df = pd.DataFrame()\n",
    "R2 = np.empty(len(SITES))\n",
    "RMSE = np.empty(len(SITES))\n",
    "\n",
    "\n",
    "for i, site in enumerate(SITES):\n",
    "    site = int(site)\n",
    "    print(\"\\n Training of model for building {}\\n\".format(site))\n",
    "    \n",
    "    training_set = pd.read_csv('training_building{}_DB.csv'.format(site), index_col=0)\n",
    "    #print(\"\\n Training set overview\\n\", training_set.head())\n",
    "    #print(training_set.info())\n",
    "  \n",
    "    #Create the features (X) and target (y) arrays for the training\n",
    "    X = training_set.drop(columns=['Value']).values\n",
    "    y = training_set.loc[:,'Value'].values.reshape((-1,1))\n",
    "\n",
    "    #print('shape of X and Y arrays: {}'.format([X.shape, y.shape]))\n",
    "\n",
    "\n",
    "    #Building the model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "    #Alpha Hyperparameter tunning\n",
    "    param_grid={'reg__alpha': np.logspace(-4,0,30)}\n",
    "\n",
    "    #Pipeline building\n",
    "    pipeline = Pipeline([('imp', Imputer(strategy='mean', axis=1)),\n",
    "                         ('reg', Ridge(normalize=True))\n",
    "                        ])\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid=param_grid, cv=10)\n",
    "    \n",
    "    #Training on the training set of the building\n",
    "    cv.fit(X_train, y_train)\n",
    "    pred_training = cv.predict(X_test)\n",
    "    score = cv.score(X_test, y_test)\n",
    "    \n",
    "    print(\"Hyperparameter fine tunning:\", cv.best_params_, cv.best_score_)\n",
    "    print('Score R² on test set: {}'.format(score))\n",
    "\n",
    "    #Simple RMSE for 1 building\n",
    "    mu = 1 / np.mean(y_test)\n",
    "    simple_rmse = mu * np.sqrt(np.sum((pred_training - y_test) ** 2))\n",
    "    print('RMSE result of training: {}'.format(simple_rmse))\n",
    "    \n",
    "    #Keep the scores (R² & RMSE) in a dataframe\n",
    "    R2[i] = score\n",
    "    RMSE[i] = simple_rmse\n",
    "    \n",
    "    \n",
    "    #Prepare the submission set for predictions\n",
    "    sub_building = submission.loc[submission['SiteId'] == site]\n",
    "    results_df = sub_building.loc[:, SUB]\n",
    "    #print(sub_building.head())\n",
    "    #print(sub_building.shape)\n",
    "\n",
    "    #Add temperature and building specific features to submission set\n",
    "    ##HERE##\n",
    "    sub_building = building_features_sub(site, sub_building)\n",
    "\n",
    "    #print(sub_building.shape)\n",
    "    sub_building = sub_building.drop(columns=['Value'])\n",
    "    #print(\"Submission table for predictions: \\n\", sub_building.head())\n",
    "\n",
    "    X_sub = sub_building.values\n",
    "    y_sub = cv.predict(X_sub)\n",
    "    \n",
    "    #Append the predictions and submission df with the obtained predictions\n",
    "    results_df['Value'] = y_sub\n",
    "    predictions_df = predictions_df.append(results_df)\n",
    "    scores_df = pd.DataFrame({\"Building\": SITES, \"R2\": R2, \"RMSE\": RMSE})\n",
    "    \n",
    "print(predictions_df.head())\n",
    "print(scores_df.head())\n",
    "predictions_df.to_csv('submission_format_results_DB.csv')\n",
    "scores_df.to_csv('scores_DB.csv')\n",
    "\n",
    "#Display the best scores obtained\n",
    "max_R2 = np.max(R2)\n",
    "min_rmse = np.min(RMSE)\n",
    "print(\"\\n Best performances of the model: R²={} and RMSE={}\".format(max_R2, min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pipeline, predicts on all building altogether\n",
    "- Load the submission file\n",
    "- \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
